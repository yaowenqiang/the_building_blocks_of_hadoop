> mkdir input
> cp etc/hadoo;/* input/
> ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+'


get java home dir on mac

> /usr/libexec/java_home

> vim etc/hadoop/hadoop-env.sh

change JAVA_HOME

> export JAVA_HOME=/Users/yaojack/Library/Java/JavaVirtualMachines/openjdk-20.0.1/Contents/Home

add HADOOP_PREFIX

> export HADOOP_PREFIX=/Users/yaojack/bigdata/hadoop-2.10.2

edit config

> vim etc/hadoop/core-site.xml
> vim etc/hadoop/hdfs-site.xml
> vim etc/hadoop/mapred-site.xml
> vim etc/hadoop/yarn-site.xml

format the name node

> ./bin/hdfs namenode -format

> ./sbin/start-dfs.sh

see node status 

> localhost:50070

create user dir

> ./bin/hdfs dfs -mkdir -p /user/yaojack

start yarn

./sbin/start-yarn.sh

monitor clusters

localhost;8088


hdfs  dfs -mkdir input
hdfs  dfs -put  etc/hadoop/* input/

>  hadoop jar   share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar grep input output 'dfs[a-z.]+'


## Replication

+ Replication blocks based on the replication factor
+ Store replicas in different locations

Choosing Replica Locations

Maximize redundancy 
Minimize write bandwidth



